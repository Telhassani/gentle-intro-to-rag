{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to RAG Applications\n",
    "\n",
    "This notebook creates a simple RAG (Retrieval-Augmented Generation) system to answer questions from a PDF document using an open-source model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"paul.pdf\"\n",
    "\n",
    "# We'll be using Llama 3.1 8B for this example.\n",
    "MODEL = \"llama3:latest\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the PDF document\n",
    "\n",
    "Let's start by loading the PDF document and breaking it down into separate pages.\n",
    "\n",
    "<img src='images/documents.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 9\n",
      "Length of a page: 3272\n",
      "Content of a page: 10% a week. And while 110 may not seem much better than 100,\n",
      "if you keep growing at 10% a week you'll be surprised how big\n",
      "the numbers get. After a year you'll have 14,000 users, and after\n",
      "2 years you'll have 2 million.\n",
      "You'll be doing different things when you're acquiring users a\n",
      "thousand at a time, and growth has to slow down eventually. But\n",
      "if the market exists you can usually start by recruiting users\n",
      "manually and then gradually switch to less manual methods. [3]\n",
      "Airbnb is a classic example of this technique. Marketplaces are so\n",
      "hard to get rolling that you should expect to take heroic measures\n",
      "at first. In Airbnb's case, these consisted of going door to door in\n",
      "New York, recruiting new users and helping existing ones improve\n",
      "their listings. When I remember the Airbnbs during YC, I picture\n",
      "them with rolly bags, because when they showed up for tuesday\n",
      "dinners they'd always just flown back from somewhere.\n",
      "Fragile\n",
      "Airbnb now seems like an unstoppable juggernaut, but early on it\n",
      "was so fragile that about 30 days of going out and engaging in\n",
      "person with users made the difference between success and\n",
      "failure.\n",
      "That initial fragility was not a unique feature of Airbnb. Almost all\n",
      "startups are fragile initially. And that's one of the biggest things\n",
      "inexperienced founders and investors (and reporters and know-it-\n",
      "alls on forums) get wrong about them. They unconsciously judge\n",
      "larval startups by the standards of established ones. They're like\n",
      "someone looking at a newborn baby and concluding \"there's no\n",
      "way this tiny creature could ever accomplish anything.\"\n",
      "It's harmless if reporters and know-it-alls dismiss your startup.\n",
      "They always get things wrong. It's even ok if investors dismiss\n",
      "your startup; they'll change their minds when they see growth.\n",
      "The big danger is that you'll dismiss your startup yourself. I've\n",
      "seen it happen. I often have to encourage founders who don't see\n",
      "the full potential of what they're building. Even Bill Gates made\n",
      "that mistake. He returned to Harvard for the fall semester after\n",
      "starting Microsoft. He didn't stay long, but he wouldn't have\n",
      "returned at all if he'd realized Microsoft was going to be even a\n",
      "fraction of the size it turned out to be. [4]\n",
      "The question to ask about an early stage startup is not \"is this\n",
      "company taking over the world?\" but \"how big could this company\n",
      "get if the founders did the right things?\" And the right things\n",
      "often seem both laborious and inconsequential at the time.\n",
      "Microsoft can't have seemed very impressive when it was just a\n",
      "couple guys in Albuquerque writing Basic interpreters for a\n",
      "market of a few thousand hobbyists (as they were then called),\n",
      "but in retrospect that was the optimal path to dominating\n",
      "microcomputer software. And I know Brian Chesky and Joe\n",
      "Gebbia didn't feel like they were en route to the big time as they\n",
      "were taking \"professional\" photos of their first hosts' apartments.\n",
      "They were just trying to survive. But in retrospect that too was\n",
      "the optimal path to dominating a big market.\n",
      "How do you find users to recruit manually? If you build something\n",
      "to solve your own problems, then you only have to find your\n",
      "peers, which is usually straightforward. Otherwise you'll have to\n",
      "8/6/24, 11:04 AM Do Things that Don't Scale\n",
      "https://paulgraham.com/ds.html 2/9\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "pages = loader.load()\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(\"Content of a page:\", pages[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the pages in chunks\n",
    "\n",
    "Pages are too long, so let's split pages into different chunks.\n",
    "\n",
    "<img src='images/splitter.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 23\n",
      "Length of a chunk: 1236\n",
      "Content of a chunk: took better advantage of it than Stripe. At YC we use the term\n",
      "\"Collison installation\" for the technique they invented. More\n",
      "diffident founders ask \"Will you try our beta?\" and if the answer is\n",
      "yes, they say \"Great, we'll send you a link.\" But the Collison\n",
      "brothers weren't going to wait. When anyone agreed to try Stripe\n",
      "they'd say \"Right then, give me your laptop\" and set them up on\n",
      "the spot.\n",
      "There are two reasons founders resist going out and recruiting\n",
      "users individually. One is a combination of shyness and laziness.\n",
      "They'd rather sit at home writing code than go out and talk to a\n",
      "bunch of strangers and probably be rejected by most of them.\n",
      "But for a startup to succeed, at least one founder (usually the\n",
      "CEO) will have to spend a lot of time on sales and marketing. [2]\n",
      "The other reason founders ignore this path is that the absolute\n",
      "numbers seem so small at first. This can't be how the big, famous\n",
      "startups got started, they think. The mistake they make is to\n",
      "underestimate the power of compound growth. We encourage\n",
      "every startup to measure their progress by weekly growth rate. If\n",
      "you have 100 users, you need to get 10 more next week to grow\n",
      "8/6/24, 11:04 AM Do Things that Don't Scale\n",
      "https://paulgraham.com/ds.html 1/9\n"
     ]
    }
   ],
   "source": [
    "# Import the RecursiveCharacterTextSplitter class for splitting text into manageable chunks.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create an instance of RecursiveCharacterTextSplitter.\n",
    "# - `chunk_size=1500`: Defines the maximum number of characters in each chunk.\n",
    "# - `chunk_overlap=100`: Specifies the number of overlapping characters between consecutive chunks, \n",
    "#   ensuring context continuity between chunks.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "\n",
    "# Use the text splitter to divide the input document(s) (`pages`) into smaller chunks.\n",
    "# `pages` is expected to be a list of documents or text data (e.g., PDF pages or paragraphs).\n",
    "chunks = splitter.split_documents(pages)\n",
    "\n",
    "# Print the total number of chunks created after splitting.\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Print the length (number of characters) of the second chunk.\n",
    "# `chunks[1].page_content` refers to the text content of the second chunk.\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "\n",
    "# Print the content of the second chunk to examine its text.\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the chunks in a vector store\n",
    "\n",
    "We can now generate embeddings for every chunk and store them in a vector store.\n",
    "\n",
    "<img src='images/vectorstore.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FAISS class for creating a vector store, which enables efficient similarity search and retrieval.\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Import the OllamaEmbeddings class, which generates vector embeddings from a specified language model.\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Create an instance of OllamaEmbeddings by specifying the model to use.\n",
    "# This model will convert text data into numerical vector representations.\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "# Create a FAISS vector store from the provided document chunks and their embeddings.\n",
    "# `chunks` is a collection of document segments (e.g., paragraphs or sentences).\n",
    "# `embeddings` is used to generate vector representations for these chunks.\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a retriever\n",
    "\n",
    "We can use a retriever to find chunks in the vector store that are similar to a supplied question.\n",
    "\n",
    "<img src='images/retriever.png' width=\"1000\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1d97e3cc-fc0f-4a46-abb0-d53ef3b7fce7', metadata={'source': 'paul.pdf', 'page': 2}, page_content=\"in charge of their narrow domain of building things, rather than\\nrunning the whole show. You can be ornery when you're Scotty,\\nbut not when you're Kirk.\\nAnother reason founders don't focus enough on individual\\ncustomers is that they worry it won't scale. But when founders of\\nlarval startups worry about this, I point out that in their current\\nstate they have nothing to lose. Maybe if they go out of their way\\nto make existing users super happy, they'll one day have too\\nmany to do so much for. That would be a great problem to have.\\nSee if you can make it happen. And incidentally, when it does,\\nyou'll find that delighting customers scales better than you\\nexpected. Partly because you can usually find ways to make\\nanything scale more than you would have predicted, and partly\\nbecause delighting customers will by then have permeated your\\nculture.\\nI have never once seen a startup lured down a blind alley by\\ntrying too hard to make their initial users happy.\\nBut perhaps the biggest thing preventing founders from realizing\\nhow attentive they could be to their users is that they've never\\nexperienced such attention themselves. Their standards for\\ncustomer service have been set by the companies they've been\\ncustomers of, which are mostly big ones. Tim Cook doesn't send\\nyou a hand-written note after you buy a laptop. He can't. But you\\ncan. That's one advantage of being small: you can provide a level\\nof service no big company can. [6]\"),\n",
       " Document(id='ed3d0a8b-07ed-46f6-80f1-3ccc61664adc', metadata={'source': 'paul.pdf', 'page': 3}, page_content='Experience\\nI was trying to think of a phrase to convey how extreme your\\nattention to users should be, and I realized Steve Jobs had\\nalready done it: insanely great. Steve wasn\\'t just using \"insanely\"\\nas a synonym for \"very.\" He meant it more literally — that one\\nshould focus on quality of execution to a degree that in everyday\\nlife would be considered pathological.\\nAll the most successful startups we\\'ve funded have, and that\\nprobably doesn\\'t surprise would-be founders. What novice\\nfounders don\\'t get is what insanely great translates to in a larval\\nstartup. When Steve Jobs started using that phrase, Apple was\\nalready an established company. He meant the Mac (and its\\ndocumentation and even packaging — such is the nature of\\nobsession) should be insanely well designed and manufactured.\\nThat\\'s not hard for engineers to grasp. It\\'s just a more extreme\\nversion of designing a robust and elegant product.\\nWhat founders have a hard time grasping (and Steve himself\\nmight have had a hard time grasping) is what insanely great\\nmorphs into as you roll the time slider back to the first couple\\nmonths of a startup\\'s life. It\\'s not the product that should be\\ninsanely great, but the experience of being your user. The product\\nis just one component of that. For a big company it\\'s necessarily\\nthe dominant one. But you can and should give users an insanely\\ngreat experience with an early, incomplete, buggy product, if you\\nmake up the difference with attentiveness.'),\n",
       " Document(id='034220fc-fa6f-412c-9ae2-48afe8c1de9e', metadata={'source': 'paul.pdf', 'page': 1}, page_content='were taking \"professional\" photos of their first hosts\\' apartments.\\nThey were just trying to survive. But in retrospect that too was\\nthe optimal path to dominating a big market.\\nHow do you find users to recruit manually? If you build something\\nto solve your own problems, then you only have to find your\\npeers, which is usually straightforward. Otherwise you\\'ll have to\\n8/6/24, 11:04 AM Do Things that Don\\'t Scale\\nhttps://paulgraham.com/ds.html 2/9'),\n",
       " Document(id='0cbcf4b4-e08f-45c9-b296-96d9329c5285', metadata={'source': 'paul.pdf', 'page': 1}, page_content='larval startups by the standards of established ones. They\\'re like\\nsomeone looking at a newborn baby and concluding \"there\\'s no\\nway this tiny creature could ever accomplish anything.\"\\nIt\\'s harmless if reporters and know-it-alls dismiss your startup.\\nThey always get things wrong. It\\'s even ok if investors dismiss\\nyour startup; they\\'ll change their minds when they see growth.\\nThe big danger is that you\\'ll dismiss your startup yourself. I\\'ve\\nseen it happen. I often have to encourage founders who don\\'t see\\nthe full potential of what they\\'re building. Even Bill Gates made\\nthat mistake. He returned to Harvard for the fall semester after\\nstarting Microsoft. He didn\\'t stay long, but he wouldn\\'t have\\nreturned at all if he\\'d realized Microsoft was going to be even a\\nfraction of the size it turned out to be. [4]\\nThe question to ask about an early stage startup is not \"is this\\ncompany taking over the world?\" but \"how big could this company\\nget if the founders did the right things?\" And the right things\\noften seem both laborious and inconsequential at the time.\\nMicrosoft can\\'t have seemed very impressive when it was just a\\ncouple guys in Albuquerque writing Basic interpreters for a\\nmarket of a few thousand hobbyists (as they were then called),\\nbut in retrospect that was the optimal path to dominating\\nmicrocomputer software. And I know Brian Chesky and Joe\\nGebbia didn\\'t feel like they were en route to the big time as they\\nwere taking \"professional\" photos of their first hosts\\' apartments.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the vector store object into a retriever.\n",
    "# - `vectorstore.as_retriever()` creates an interface to retrieve relevant information from the vector store.\n",
    "# - The retriever uses the embeddings in the vector store to find the most relevant documents based on the query.\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Invoke the retriever with a specific query.\n",
    "# - The query (\"What can you get away with when you only have a small number of users?\") is used to find the most relevant\n",
    "#   information stored in the vector store.\n",
    "# - The retriever processes the query using the stored embeddings and returns the closest matching content.\n",
    "retriever.invoke(\"What can you get away with when you only have a small number of users?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the model\n",
    "\n",
    "We'll be using Ollama to load the local model in memory. After creating the model, we can invoke it with a question to get the response back.\n",
    "\n",
    "<img src='images/model.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As of', additional_kwargs={}, response_metadata={'model': 'llama3:latest', 'created_at': '2024-12-31T10:42:28.982217Z', 'done': True, 'done_reason': 'length', 'total_duration': 1748837708, 'load_duration': 34473292, 'prompt_eval_count': 19, 'prompt_eval_duration': 1680000000, 'eval_count': 2, 'eval_duration': 33000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c99ab300-17f6-4357-8d44-ca15b5b1d53a-0', usage_metadata={'input_tokens': 19, 'output_tokens': 2, 'total_tokens': 21})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the ChatOllama class from the langchain_ollama module.\n",
    "# - ChatOllama is a wrapper designed to interact with the Ollama model, which is used for conversational AI tasks.\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the ChatOllama model with specific parameters.\n",
    "# - `model=MODEL`: Specifies the name or identifier of the Ollama model to be used.\n",
    "# - `temperature=0`: Sets the temperature parameter, which controls the randomness of the output.\n",
    "#    - A temperature of 0 ensures deterministic responses (always provides the same output for the same input).\n",
    "model = ChatOllama(model=MODEL, temperature=0, num_predict=2)\n",
    "\n",
    "# Invoke the ChatOllama model with a query.\n",
    "# - The `.invoke()` method sends a query to the model for processing.\n",
    "# - \"Who is the president of the United States?\" is the input query that the model processes.\n",
    "# - The model generates and returns a response based on the knowledge encoded within it.\n",
    "model.invoke(\"Who is the president of the United States?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the model's response\n",
    "\n",
    "The response from the model is an `AIMessage` instance containing the answer. We can extract the text answer by using the appropriate output parser. We can connect the model and the parser using a chain.\n",
    "\n",
    "<img src='images/parser.png' width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of\n"
     ]
    }
   ],
   "source": [
    "# Import the StrOutputParser class from the langchain_core.output_parsers module.\n",
    "# - StrOutputParser is used to process and parse the output from a model into a string format.\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the StrOutputParser to parse the output from the model.\n",
    "# - StrOutputParser will convert the output of the model into a clean and structured string format.\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define a 'chain' that combines the model and the output parser.\n",
    "# - The pipe (`|`) operator is used to create a sequence of steps, where the model's output is passed directly to the parser.\n",
    "# - This means the model will first generate the response, and then the parser will process it into a string.\n",
    "chain = model | parser \n",
    "\n",
    "# Invoke the chain with a query.\n",
    "# - The `.invoke()` method is used to send the query (\"Who is the president of the United States?\") through the chain.\n",
    "# - The model generates a response, which is then parsed by the StrOutputParser, and the final result is printed.\n",
    "print(chain.invoke(\"Who is the president of the United States?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a prompt\n",
    "\n",
    "In addition to the question we want to ask, we also want to provide the model with the context from the PDF file. We can use a prompt template to define and reuse the prompt we'll use with the model.\n",
    "\n",
    "\n",
    "<img src='images/prompt.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on\n",
      "a given context. \n",
      "\n",
      "Answer the question based on the context. If you can't answer the\n",
      "question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on\n",
    "a given context. \n",
    "\n",
    "Answer the question based on the context. If you can't answer the\n",
    "question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the prompt to the chain\n",
    "\n",
    "We can now chain the prompt with the model and the parser.\n",
    "\n",
    "<img src='images/chain1.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Anna.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\": \"Anna's sister is Susan\", \n",
    "    \"question\": \"Who is Susan's sister?\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the retriever to the chain\n",
    "\n",
    "Finally, we can connect the retriever to the chain to get the context from the vector store.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the itemgetter function from the operator module.\n",
    "# - itemgetter is used to retrieve specific items (e.g., keys or values) from dictionaries or lists.\n",
    "from operator import itemgetter\n",
    "\n",
    "# Define a 'chain' that combines multiple steps into a sequence using the pipe operator (`|`).\n",
    "# - Each step in the chain processes the data and passes it to the next component.\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # This step retrieves the value associated with the \"question\" key from a dictionary and passes it to the 'retriever'.\n",
    "        # itemgetter(\"question\") is used to extract the value for the \"question\" key from a dictionary-like structure.\n",
    "        # The 'retriever' is likely a function or model that uses the question to retrieve relevant information or context.\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \n",
    "        # This step retrieves the value of the \"question\" key and passes it along as-is.\n",
    "        # The \"question\" is passed directly to the next component (prompt) without any transformation.\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    # Now that the dictionary is set up with context and question retrieval, the following steps in the chain are executed.\n",
    "    # The sequence of components is as follows: the dictionary goes through the 'prompt', 'model', and 'parser'.\n",
    "    | prompt        # The 'prompt' is likely a step where a prompt template or format is applied to the input data.\n",
    "    | model         # The 'model' generates a response based on the prompt and context (possibly a language model).\n",
    "    | parser        # The 'parser' processes and formats the output from the model (likely into a structured string).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the chain to answer questions\n",
    "\n",
    "Finally, we can use the chain to ask questions that will be answered using the PDF document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What can you get away with when you only have a small number of users?\n",
      "Answer: You can\n",
      "*************************\n",
      "\n",
      "Question: What's the most common unscalable thing founders have to do at the start?\n",
      "Answer: According to\n",
      "*************************\n",
      "\n",
      "Question: What's one of the biggest things inexperienced founders and investors get wrong about startups?\n",
      "Answer: I don\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What can you get away with when you only have a small number of users?\",\n",
    "    \"What's the most common unscalable thing founders have to do at the start?\",\n",
    "    \"What's one of the biggest things inexperienced founders and investors get wrong about startups?\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
